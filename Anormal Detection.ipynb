{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2714cb",
   "metadata": {},
   "source": [
    "# Anormal Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159dc72",
   "metadata": {},
   "source": [
    "Anomaly detection is a machine learning technique used to identify rare items, events, or observations in a dataset that differ significantly from the majority of the data. Anomaly detection can be used in a variety of business use cases, such as fraud detection in financial data, intrusion detection in system security, and identifying multivariate outliers in a dataset.\n",
    "\n",
    "In fraud detection, anomaly detection algorithms can be used to detect unusual credit card transactions or insurance claims, which may indicate fraudulent activity. In intrusion detection, anomaly detection algorithms can be used to detect unusual patterns in network traffic, such as spikes in traffic or unusual activity that may indicate a security breach. And in identifying multivariate outliers, anomaly detection algorithms can be used to identify data points that are significantly different from the rest of the data, which can be useful for identifying structural defects, medical problems, or errors in a dataset.\n",
    "\n",
    "Anomaly detection algorithms typically work by first training on a dataset of normal, or \"inlier\" data, in order to learn what patterns are typical in the data. Then, when presented with new data, the algorithm can identify any instances that do not fit these typical patterns as potential anomalies.\n",
    "\n",
    "There are several types of anomaly detection algorithms, including statistical methods such as Gaussian mixture models, clustering-based methods such as k-nearest neighbors, and machine learning methods such as support vector machines and neural networks.\n",
    "\n",
    "One important consideration when using anomaly detection is the tradeoff between false positives and false negatives. False positives occur when the algorithm flags an instance as anomalous when it is actually normal, while false negatives occur when the algorithm fails to flag an instance as anomalous when it is in fact an outlier. The choice of algorithm and threshold values can affect this tradeoff.\n",
    "\n",
    "Another consideration is the choice of features used to train the algorithm. Anomaly detection algorithms are sensitive to the choice of features, and may perform poorly if the features do not capture the relevant aspects of the data that distinguish normal and anomalous instances.\n",
    "\n",
    "Finally, it's worth noting that anomaly detection is not a silver bullet for identifying all types of problems in a dataset. It is most effective when used in combination with other techniques and domain knowledge to gain a more comprehensive understanding of the data and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659e519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e995347",
   "metadata": {},
   "source": [
    "# Fraud detection (credit cards, insurance, etc.) using financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3056a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('financial_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can then create a model using the EllipticEnvelope algorithm, which is commonly used for anomaly detection:\n",
    "clf = EllipticEnvelope(contamination=0.01)  # Contamination is the expected proportion of outliers in the dataset\n",
    "clf.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafc109",
   "metadata": {},
   "source": [
    "The predicted values will be either 1 or -1, with -1 indicating an anomaly. We can then filter the original DataFrame to show only the anomalous data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = data[predictions == -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now perform some preprocessing on the data, such as removing any missing values and scaling the data:\n",
    "data = data.dropna()\n",
    "data = (data - data.mean()) / data.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now use the Isolation Forest algorithm to detect any anomalies in the data. This algorithm is useful for detecting outliers in high-dimensional datasets.\n",
    "model = IsolationForest(n_estimators=100, contamination=0.01)\n",
    "model.fit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f64f7c",
   "metadata": {},
   "source": [
    "Here, we have set the number of trees in the forest to 100 and the contamination parameter to 0.01, which means we expect approximately 1% of the data to be anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faca9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we can predict the anomalies in the data using the trained model:\n",
    "anomalies = model.predict(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f898d57",
   "metadata": {},
   "source": [
    "The anomalies variable will contain an array of -1 and 1 values, where -1 indicates an anomaly and 1 indicates a normal data point.\n",
    "\n",
    "We can now use these anomaly predictions to flag any suspicious transactions or events for further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a53d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c006887",
   "metadata": {},
   "source": [
    "# Intrusion detection (system security, malware) or monitoring for network traffic surges and drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('network_traffic.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ddb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now perform some preprocessing on the data, such as removing any missing values and scaling the data:\n",
    "data = data.dropna()\n",
    "data = (data - data.mean()) / data.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now use the Isolation Forest algorithm to detect any anomalies in the data. This algorithm is useful for detecting outliers in high-dimensional datasets.\n",
    "model = IsolationForest(n_estimators=100, contamination=0.01)\n",
    "model.fit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18d8fc",
   "metadata": {},
   "source": [
    "Here, we have set the number of trees in the forest to 100 and the contamination parameter to 0.01, which means we expect approximately 1% of the data to be anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we can predict the anomalies in the data using the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = model.predict(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c55384",
   "metadata": {},
   "source": [
    "The anomalies variable will contain an array of -1 and 1 values, where -1 indicates an anomaly and 1 indicates a normal data point.\n",
    "\n",
    "We can now use these anomaly predictions to flag any suspicious network traffic events for further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3996ead0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88845645",
   "metadata": {},
   "source": [
    "# Identifying multivariate outliers in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d565e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401dd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now perform some preprocessing on the data, such as removing any missing values and scaling the data:\n",
    "data = (data - data.mean()) / data.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07300c2f",
   "metadata": {},
   "source": [
    "We will now use the Mahalanobis distance to detect any multivariate outliers in the data. The Mahalanobis distance is a measure of the distance between a point and a distribution, taking into account the covariance of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1327ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov = np.cov(data.T)\n",
    "\n",
    "# Calculate the inverse covariance matrix\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "# Calculate the squared Mahalanobis distances\n",
    "squared_distances = []\n",
    "for i in range(len(data)):\n",
    "    x = data[i]\n",
    "    diff = x - data.mean(axis=0)\n",
    "    squared_distance = diff.dot(inv_cov).dot(diff.T)\n",
    "    squared_distances.append(squared_distance)\n",
    "\n",
    "# Calculate the p-values for each data point\n",
    "p_values = 1 - chi2.cdf(squared_distances, df=len(data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e7c94",
   "metadata": {},
   "source": [
    "Here, we have calculated the Mahalanobis distances for each data point, and then used these distances to calculate the p-values for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b69bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435cbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07b351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
